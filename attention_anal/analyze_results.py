"""
PHASE 2: RESULTS ANALYZER

This script loads model checkpoints generated by `run_all_experiments.py`
and performs attention and activation analysis.

It reads an `experiment_manifest.json` file to know which runs to analyze.
"""
import os
import sys
import json
import argparse
import torch

# Add mammoth path to system path
mammoth_path = os.path.dirname(os.path.abspath(__file__))
if mammoth_path not in sys.path:
    sys.path.insert(0, mammoth_path)

# Imports for analysis
from backbone import get_backbone
from datasets import get_dataset
from models import get_model
from utils.attention_visualization import analyze_task_attention
from utils.network_flow_visualization import (
    ActivationExtractor,
    visualize_activations,
)


def analyze_run(run_info: dict, base_args: dict, results_dir: str):
    """Analyzes a single successful experiment run."""
    method = run_info["method"]
    seed = run_info["seed"]
    ckpt_prefix = run_info["ckpt_name"]
    print(f"\n--- Analyzing Method: {method}, Seed: {seed} ---")

    # Find all checkpoint files for this run in the global checkpoints directory
    global_checkpoint_dir = os.path.join(mammoth_path, "checkpoints")
    if not os.path.exists(global_checkpoint_dir):
        print(f"✗ Global checkpoint directory not found: {global_checkpoint_dir}")
        return

    checkpoints = sorted(
        [
            f
            for f in os.listdir(global_checkpoint_dir)
            if f.startswith(ckpt_prefix) and f.endswith((".pth", ".pt"))
        ]
    )

    if not checkpoints:
        print(f"✗ No checkpoints found starting with prefix '{ckpt_prefix}'")
        return

    print(f"Found checkpoints for analysis: {checkpoints}")

    # Prepare a complete Namespace for model loading
    args_dict = base_args.copy()
    args_dict.update({"model": method, "seed": seed, "device": "cuda" if torch.cuda.is_available() else "cpu"})
    if method == "derpp":
        args_dict.update({"buffer_size": 200})
    args = argparse.Namespace(**args_dict)

    try:
        dataset = get_dataset(args)
        backbone = get_backbone(args)
        loss = dataset.get_loss()
        transform = dataset.get_transform()
        model = get_model(args, backbone, loss, transform, dataset)
        model.to(args.device)

        for ckpt_file in checkpoints:
            task_id_str = os.path.splitext(ckpt_file)[0].split("_")[-1]
            task_id = int(task_id_str)
            print(f"  > Analyzing checkpoint: {ckpt_file} (after Task {task_id})")

            checkpoint_path = os.path.join(global_checkpoint_dir, ckpt_file)
            state_dict = torch.load(checkpoint_path, map_location=args.device)
            model.load_state_dict(state_dict["net"])

            # Define where to save the analysis output for this checkpoint
            analysis_dir = os.path.join(
                results_dir, f"{method}_seed_{seed}", "analysis", f"after_task_{task_id}"
            )
            os.makedirs(analysis_dir, exist_ok=True)

            # Analyze all tasks seen so far
            for past_task_id in range(task_id + 1):
                print(f"    - Visualizing attention for Task {past_task_id} samples...")
                dataset.set_task(past_task_id)
                task_analysis_dir = os.path.join(
                    analysis_dir, f"analyzing_task_{past_task_id}"
                )

                analyze_task_attention(
                    model,
                    dataset,
                    device=args.device,
                    save_dir=os.path.join(task_analysis_dir, "attention"),
                )
    except Exception as e:
        print(f"✗ Error during analysis for {method} seed {seed}: {e}")
        import traceback
        traceback.print_exc()


def main():
    """Main function to load manifest and run analysis."""
    parser = argparse.ArgumentParser(description="Analyze shortcut investigation results.")
    parser.add_argument(
        "--results_dir",
        type=str,
        required=True,
        help="Path to the experiment results directory containing the manifest file.",
    )
    args = parser.parse_args()

    manifest_path = os.path.join(args.results_dir, "experiment_manifest.json")
    if not os.path.exists(manifest_path):
        print(f"✗ Error: Manifest file not found at {manifest_path}")
        return

    with open(manifest_path, "r") as f:
        manifest = json.load(f)

    print(f"Loaded manifest for experiment: {manifest['experiment_name']}")
    base_args = manifest["base_args"]

    for run_info in manifest["runs"]:
        if run_info["status"] == "success":
            analyze_run(run_info, base_args, args.results_dir)
        else:
            print(f"\n--- Skipping failed run: {run_info['method']} seed {run_info['seed']} ---")

    print("\n" + "=" * 50)
    print("Analysis complete.")
    print(f"Visualizations saved in subdirectories of: {args.results_dir}")
    print("=" * 50)


if __name__ == "__main__":
    main()