# Continual Learning Attention Investigation Experiment

This experiment investigates shortcut features in continual learning using attention visualization on Vision Transformers (ViT). The experiment compares DER++ and EWC methods on a custom 2-task CIFAR-10 setup designed to study potential shortcut learning.

## Experiment Design

### Task Structure
- **Task 1**: airplane (0), automobile (1) - potential shortcuts: sky, road
- **Task 2**: bird (2), truck (9) - potential shortcuts: sky, road/wheels

This design allows investigation of:
- **Sky shortcuts**: Shared between airplane and bird
- **Road shortcuts**: Shared between automobile and truck
- **Cross-task interference**: How shortcuts learned in Task 1 affect Task 2

### Methods Compared
- **DER++**: Memory-based continual learning with experience replay
- **EWC**: Regularization-based continual learning with elastic weight consolidation

### Architecture
- **Backbone**: Vision Transformer (ViT) for rich attention map extraction
- **Input size**: 224x224 (standard for ViT)
- **Attention heads**: Multiple heads analyzed per transformer block

## Files Overview

### Core Experiment Files
- `run_attention_experiment.py` - Main experiment runner with attention analysis
- `test_attention_setup.py` - Setup verification script
- `run_quick_test.py` - Quick pipeline test (minimal epochs)
- `analyze_attention_results.py` - Comprehensive results analysis

### Supporting Files
- `datasets/seq_cifar10_224_custom.py` - Custom dataset implementation
- `experiments/shortcut_investigation.py` - Original experiment framework
- `utils/attention_visualization.py` - Attention extraction and visualization
- `utils/network_flow_visualization.py` - Network activation analysis

## Quick Start

### 1. Verify Setup
```bash
python test_attention_setup.py
```
This tests:
- Custom dataset loading
- ViT backbone initialization
- DER++ and EWC model creation
- Attention visualization components

### 2. Run Quick Test
```bash
python run_quick_test.py
```
This runs a minimal experiment (2 epochs) to verify the full pipeline works.

### 3. Run Full Experiment
```bash
# Default settings (15 epochs per task, both methods, 3 seeds)
python run_attention_experiment.py

# Custom settings
python run_attention_experiment.py --epochs 20 --methods derpp ewc_on --seeds 42 123 456

# CPU only (for testing)
python run_attention_experiment.py --cpu_only --epochs 5
```

### 4. Analyze Results
```bash
python analyze_attention_results.py results/attention_exp_TIMESTAMP/
```

## Experiment Configuration

### Default Parameters
```python
EXPERIMENT_CONFIG = {
    "methods": ['derpp', 'ewc_on'],
    "seeds": [42, 123, 456],  # Fixed seeds for reproducibility
    "epochs_per_task": 15,
    "batch_size": 32,
    "lr": 0.01,
    "optimizer": "sgd",
    "dataset": "seq-cifar10-224-custom",
    "backbone": "vit",
}
```

### Reproducibility
- **Fixed random seeds** across all experiments
- **Deterministic CUDA operations** when using GPU
- **Same initialization** for fair comparison between methods

## Output Structure

```
results/attention_exp_TIMESTAMP/
├── experiment_results.json          # Main results file
├── derpp_seed_42/                   # DER++ results for seed 42
│   ├── model.pth                    # Trained model weights
│   ├── attention_analysis/          # Attention analysis
│   │   ├── attention_maps/          # Individual attention visualizations
│   │   │   ├── airplane_sample_0_block_0_head_0.png
│   │   │   ├── automobile_sample_0_block_0_head_0.png
│   │   │   └── ...
│   │   └── activations/             # Network activation analysis
│   └── results.csv                  # Performance metrics
├── ewc_on_seed_42/                  # EWC results for seed 42
│   └── ...                          # Similar structure
└── analysis/                        # Generated by analyze_attention_results.py
    ├── performance_comparison.png   # Method comparison plots
    ├── attention_comparison.png     # Attention pattern analysis
    ├── performance_statistics.csv   # Numerical results
    ├── shortcut_analysis.json       # Shortcut feature analysis
    └── summary_report.json          # Comprehensive summary
```

## Key Analysis Features

### 1. Attention Map Visualization
- **Per-class attention maps**: Visualize what the model focuses on for each class
- **Multi-head analysis**: Compare attention patterns across different heads
- **Layer-wise progression**: Track attention evolution through transformer blocks

### 2. Performance Comparison
- **Class-IL vs Task-IL**: Compare both incremental learning settings
- **Statistical analysis**: Mean, std, min, max across multiple seeds
- **Method comparison**: Direct comparison between DER++ and EWC

### 3. Shortcut Investigation
- **Spatial attention analysis**: Identify regions of high attention
- **Cross-task comparison**: Compare attention patterns between related classes
- **Shortcut hypothesis testing**: Analyze sky/road region attention

## Expected Findings

### Attention Patterns
- **Airplane vs Bird**: Both may show high attention to sky regions (upper image)
- **Automobile vs Truck**: Both may show high attention to road/wheel regions (lower image)
- **Method differences**: DER++ vs EWC may show different attention concentration patterns

### Performance Implications
- **Shortcut reliance**: Methods relying heavily on shortcuts may show catastrophic forgetting
- **Attention diversity**: Methods with more diverse attention may generalize better
- **Task interference**: Shared shortcuts may cause positive or negative transfer

## Troubleshooting

### Common Issues
1. **CUDA out of memory**: Use `--cpu_only` flag or reduce batch size
2. **Dataset not found**: Ensure CIFAR-10 data is downloaded to `./data/`
3. **Import errors**: Verify all mammoth dependencies are installed
4. **Visualization errors**: Check matplotlib backend settings

### Debug Mode
```bash
# Run with minimal settings for debugging
python run_attention_experiment.py --epochs 1 --methods derpp --seeds 42 --cpu_only
```

## Next Steps

After running the experiment:

1. **Visual inspection**: Examine attention maps in `attention_maps/` directories
2. **Quantitative analysis**: Review statistics in `analysis/` directory
3. **Shortcut mitigation**: Consider attention-based regularization techniques
4. **Extended analysis**: Implement spatial attention region analysis
5. **Method development**: Design attention-aware continual learning methods

## Citation

If you use this experiment setup, please cite the original Mammoth framework:
```bibtex
@article{mammoth2022,
  title={Mammoth: An Extensible Framework for Continual Learning},
  author={Bonicelli, Lorenzo and Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Calderara, Simone},
  journal={arXiv preprint arXiv:2205.16383},
  year={2022}
}
```

## Contact

For questions about this experiment setup, please refer to the original Mammoth documentation or create an issue in the repository.
